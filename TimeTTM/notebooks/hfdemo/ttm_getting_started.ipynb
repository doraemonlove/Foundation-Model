{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7478e0e2-b7af-4fd4-b44e-ca58e0c31b71",
   "metadata": {},
   "source": [
    "# Getting started with TinyTimeMixer (TTM)\n",
    "\n",
    "This notebooke demonstrates the usage of a pre-trained `TinyTimeMixer` model for several multivariate time series forecasting tasks. For details related to model architecture, refer to the [TTM paper](https://arxiv.org/pdf/2401.03955.pdf).\n",
    "\n",
    "In this example, we will use a pre-trained TTM-512-96 model. That means the TTM model can take an input of 512 time points (`context_length`), and can forecast upto 96 time points (`forecast_length`) in the future. We will use the pre-trained TTM in two settings:\n",
    "1. **Zero-shot**: The pre-trained TTM will be directly used to evaluate on the `test` split of the target data. Note that the TTM was NOT pre-trained on the target data.\n",
    "2. **Few-shot**: The pre-trained TTM will be quickly fine-tuned on only 5% of the `train` split of the target data, and subsequently, evaluated on the `test` part of the target data.\n",
    "\n",
    "Note: Alternatively, this notebook can be modified to try the TTM-1024-96 or TTM-1536-96 model.\n",
    "\n",
    "Pre-trained TTM models will be fetched from the [Hugging Face TTM Model Repository](ibm-granite/granite-timeseries-ttm-r2).\n",
    "\n",
    "1. TTM-R1 pre-trained models can be found here: [TTM-R1 Model Card](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1)\n",
    "    1. For 512-96 model set `TTM_MODEL_REVISION=\"main\"`\n",
    "    2. For 1024-96 model set `TTM_MODEL_REVISION=\"1024_96_v1\"`\n",
    "2. TTM-R2 pre-trained models can be found here: [TTM-R2 Model Card](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2)\n",
    "    1. For 512-96 model set `TTM_MODEL_REVISION=\"main\"`\n",
    "    2. For 1024-96 model set `TTM_MODEL_REVISION=\"1024-96-r2\"`\n",
    "    3. For 1536-96 model set `TTM_MODEL_REVISION=\"1536-96-r2\"`\n",
    "\n",
    "Details about the revisions (R1 and R2) can be found [here](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab69d3f-a4e4-427a-8c8d-36c82b305855",
   "metadata": {},
   "source": [
    "## Install `tsfm` \n",
    "**[Optional for Local Run / Mandatory for Google Colab]**  \n",
    "Run the below cell to install `tsfm`. Skip if already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c23095-302a-4412-8fc9-a4143ca4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.12\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f0358-1b55-4f45-b1e0-57d2c3e5d904",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d50d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 指定使用第0个GPU\n",
    "print(torch.cuda.device_count())  # 输出可用的 GPU 数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63ae353-96df-4380-89f6-1e6cebf684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn as nn\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "\n",
    "from tsfm_public import TinyTimeMixerForPrediction, TrackingCallback, count_parameters, load_dataset\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.visualization import plot_predictions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894ac389-94e4-4956-8d09-6509d9d452e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "092f5fa8-7f21-46d5-8356-2f313276d345",
   "metadata": {},
   "source": [
    "### Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a826c4f3-1c6c-4088-b6af-f430f45fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# TTM Revision (1 or 2)\n",
    "TTM_REVISION = 2\n",
    "\n",
    "# Context length, Or Length of the history.\n",
    "# Currently supported values are: 512/1024/1536 for TTM-R-2, and 512/1024 for TTM-R1\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "FORECAST_LENGTH = 96 \n",
    "\n",
    "# Dataset\n",
    "# The dataloaders will utilize the easy-to-use YAML configurations defined below.\n",
    "# Dataset configuration YAMLS: https://github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/resources/data_config\n",
    "# Note that `dataset_root_path` can also be provided instead of `dataset_path` to the `load_dataset()` function to\n",
    "# run this notebook on already downloaded dataset.\n",
    "# Check the `load_dataset()` function to see more functionalities.\n",
    "# TARGET_DATASET = \"etth1\"\n",
    "TARGET_DATASET = \"electricity\"\n",
    "# DATASET_PATH = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
    "DATASET_PATH = \"/home/zhupengtian/zhangqingliang/granite-tsfm/datasets/electricity/electricity.csv\"\n",
    "\n",
    "# Results dir\n",
    "# OUT_DIR = \"ttm_finetuned_models/\"\n",
    "OUT_DIR = \"/home/zhupengtian/zhangqingliang/granite-tsfm/ttm_finetuned_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e255508-17c3-468b-8feb-6dcb80c67503",
   "metadata": {},
   "source": [
    "#### Automatically set TTM_MODEL_PATH and TTM_MODEL_REVISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915ea800-83d8-49dd-9f49-e4b0e209552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- TTM model path -----\n",
    "if TTM_REVISION == 1:\n",
    "    TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
    "    # ----- TTM model branch -----\n",
    "    # For R1 models\n",
    "    if CONTEXT_LENGTH == 512:\n",
    "        TTM_MODEL_REVISION = \"main\"\n",
    "    elif CONTEXT_LENGTH == 1024:\n",
    "        TTM_MODEL_REVISION = \"1024_96_v1\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported CONTEXT_LENGTH for TTM_MODEL_PATH={TTM_MODEL_PATH}\")\n",
    "elif TTM_REVISION == 2:\n",
    "    TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "    # ----- TTM model branch -----\n",
    "    # For R2 models\n",
    "    if CONTEXT_LENGTH == 512:\n",
    "        TTM_MODEL_REVISION = \"main\"\n",
    "    elif CONTEXT_LENGTH == 1024:\n",
    "        TTM_MODEL_REVISION = \"1024-96-r2\"\n",
    "    elif CONTEXT_LENGTH == 1536:\n",
    "        TTM_MODEL_REVISION = \"1536-96-r2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported CONTEXT_LENGTH for TTM_MODEL_PATH={TTM_MODEL_PATH}\")\n",
    "else:\n",
    "    raise ValueError(\"Wrong TTM_REVISION. Stay tuned for future models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525dea1-e4f1-40ee-bfcc-5cb3bc00644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chosen TTM model:\")\n",
    "print(f\"{TTM_MODEL_PATH}, revision = {TTM_MODEL_REVISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印数据集信息\n",
    "dataset = load_dataset('electricity', context_length=512, forecast_length=96, dataset_path=DATASET_PATH)\n",
    "# 直接打印数据集的类型和内容\n",
    "print(type(dataset))\n",
    "print(dataset)  # 打印 dataset 的内容\n",
    "# 假设你的数据集是一个元组，包含训练集、验证集和测试集\n",
    "train_dataset, val_dataset, test_dataset = dataset  # dataset 是加载的数据集\n",
    "\n",
    "# 获取测试集的一个示例\n",
    "test_index = 0  # 你可以修改为需要的索引\n",
    "test_sample = test_dataset[test_index]\n",
    "\n",
    "# 打印测试集示例的一些信息\n",
    "print(\"测试集数据:\")\n",
    "print(\"过去值:\", test_sample['past_values'])\n",
    "print(\"未来值:\", test_sample['future_values'])\n",
    "print(\"过去观测掩码:\", test_sample['past_observed_mask'])\n",
    "print(\"时间戳:\", test_sample['timestamp'])\n",
    "print(\"ID:\", test_sample['id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9498749",
   "metadata": {},
   "source": [
    "## Zero-shot evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7935d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_eval(dataset_name, batch_size, context_length=512, forecast_length=96, prediction_filter_length=None):\n",
    "    if prediction_filter_length is not None:\n",
    "        if prediction_filter_length >= forecast_length:\n",
    "            raise ValueError(\n",
    "                \"`prediction_filter_length` should be less than the original `forecast_length` of the pre-trained TTM model.\"\n",
    "            )\n",
    "        forecast_length = forecast_length - prediction_filter_length\n",
    "\n",
    "    # Get data\n",
    "    _, _, dset_test = load_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        context_length=context_length,\n",
    "        forecast_length=forecast_length,\n",
    "        fewshot_fraction=1.0,\n",
    "        dataset_path=DATASET_PATH,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    if prediction_filter_length is None:\n",
    "        zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(TTM_MODEL_PATH, revision=TTM_MODEL_REVISION)\n",
    "    else:\n",
    "        if prediction_filter_length <= forecast_length:\n",
    "            zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=temp_dir,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "    )\n",
    "    # evaluate = zero-shot performance\n",
    "    print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    print(zeroshot_output)\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=zeroshot_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_zeroshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b474d86",
   "metadata": {},
   "source": [
    "## Example: downstream target dataset - etth1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0d40b66",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84d458-76ca-4e2a-a756-59981e9847f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_eval(dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b90d78",
   "metadata": {},
   "source": [
    " ## Few-shot finetune and evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_finetune_eval(\n",
    "    dataset_name,\n",
    "    batch_size,\n",
    "    learning_rate=None,\n",
    "    context_length=512,\n",
    "    forecast_length=96,\n",
    "    fewshot_percent=5,\n",
    "    freeze_backbone=True,\n",
    "    num_epochs=50,\n",
    "    save_dir=OUT_DIR,\n",
    "    prediction_filter_length=None,\n",
    "):\n",
    "    out_dir = os.path.join(save_dir, dataset_name)\n",
    "\n",
    "    print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "\n",
    "    if prediction_filter_length is not None:\n",
    "        if prediction_filter_length >= forecast_length:\n",
    "            raise ValueError(\n",
    "                \"`prediction_filter_length` should be less than the original `forecast_length` of the pre-trained TTM model.\"\n",
    "            )\n",
    "        forecast_length = forecast_length - prediction_filter_length\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "    dset_train, dset_val, dset_test = load_dataset(\n",
    "        dataset_name,\n",
    "        context_length,\n",
    "        forecast_length,\n",
    "        fewshot_fraction=fewshot_percent / 100,\n",
    "        dataset_path=DATASET_PATH,\n",
    "    )\n",
    "\n",
    "    # change head dropout to 0.7 for ett datasets\n",
    "    if \"ett\" in dataset_name:\n",
    "        if prediction_filter_length is None:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH, revision=TTM_MODEL_REVISION, head_dropout=0.7\n",
    "            )\n",
    "        elif prediction_filter_length <= forecast_length:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                head_dropout=0.7,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    else:\n",
    "        if prediction_filter_length is None:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "            )\n",
    "        elif prediction_filter_length <= forecast_length:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    if freeze_backbone:\n",
    "        print(\n",
    "            \"Number of params before freezing backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "        # Freeze the backbone of the model\n",
    "        for param in finetune_forecast_model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Count params\n",
    "        print(\n",
    "            \"Number of params after freezing the backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "    # Find optimal learning rate\n",
    "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
    "    if learning_rate is None:\n",
    "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
    "            finetune_forecast_model,\n",
    "            dset_train,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
    "\n",
    "    print(f\"Using learning rate = {learning_rate}\")\n",
    "    finetune_forecast_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"output\"),\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        dataloader_num_workers=8,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create the early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
    "    )\n",
    "    tracking_callback = TrackingCallback()\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
    "    )\n",
    "\n",
    "    finetune_forecast_trainer = Trainer(\n",
    "        model=finetune_forecast_model,\n",
    "        args=finetune_forecast_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val,\n",
    "        callbacks=[early_stopping_callback, tracking_callback],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
    "\n",
    "    # Fine tune\n",
    "    finetune_forecast_trainer.train()\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
    "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "    print(fewshot_output)\n",
    "    print(\"+\" * 60)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=finetune_forecast_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_fewshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e825cf28-f034-4a32-a729-0fe846ff2a26",
   "metadata": {},
   "source": [
    "### Few-shot 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145fead-50fb-4e3e-89fc-a0c238755e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, batch_size=64, fewshot_percent=5, learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee2dcc1-bcb8-47ee-8ba7-ff3104159ed6",
   "metadata": {},
   "source": [
    "## Example: Automatically truncating the forecast horizon\n",
    "\n",
    "Here, we demonstrate that a pre-trained 512-96 TTM model (i.e., context length = 512, forecast horizon = 96) \n",
    "can be used for a task having forecast horizon less than 96 time points.\n",
    "We need to specify the argument `prediction_filter_length` while loading the model. That's it!\n",
    "\n",
    "Note that the model performance might be sacrificed by some margin while truncating the model forecast. It is recommended to try \n",
    "this feature in your validation data for your experiment, to verify if the model performance is in the acceptable threshold. \n",
    "Otherwise, a new TTM model can be pre-trained with the required forecast horizon.\n",
    "\n",
    "In this example, we will use a 512-96 TTM and use it on etth1 data for forecasting 48 points in both zero-shot and 5% few-shot settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31b58250-91a2-4db0-ab45-29b89f5afd0e",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eff2e1-acfd-4c5b-8463-e084ba831cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_eval(dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, batch_size=64, prediction_filter_length=48)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00e56d63-5e87-41ae-8fe9-3b3cfd9d19f6",
   "metadata": {},
   "source": [
    "### Few-shot 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56cd24-bae6-4cc6-9a3c-52f965014eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    batch_size=64,\n",
    "    prediction_filter_length=48,\n",
    "    fewshot_percent=5,\n",
    "    learning_rate=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zql_ttm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

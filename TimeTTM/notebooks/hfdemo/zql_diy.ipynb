{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import shutil\n",
    "import subprocess  # 用于调用 nvidia-smi 获取 GPU 内存信息\n",
    "import threading\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn as nn\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "\n",
    "from tsfm_public import TinyTimeMixerForPrediction, TrackingCallback, count_parameters, load_dataset\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.visualization import plot_predictions\n",
    "import numpy as np\n",
    "\n",
    "# 设定镜像网站\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# 查看GPU情况\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # 指定使用第0个GPU\n",
    "print(torch.cuda.device_count())  # 输出可用的 GPU 数量 # 输出可用的 GPU 数量\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "# TTM Revision (1 or 2)\n",
    "TTM_REVISION = 2\n",
    "#上下文和预测长度\n",
    "CONTEXT_LENGTH = 512\n",
    "FORECAST_LENGTH = 96 \n",
    "# Dataset\n",
    "TARGET_DATASET = \"electricity\"\n",
    "DATASET_PATH = \"/home/zhupengtian/zhangqingliang/granite-tsfm/datasets/electricity/electricity.csv\"\n",
    "# Results dir\n",
    "OUT_DIR = \"/home/zhupengtian/zhangqingliang/granite-tsfm/ttm_finetuned_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen TTM model:\n",
      "ibm-granite/granite-timeseries-ttm-r2, revision = main\n"
     ]
    }
   ],
   "source": [
    "# ----- TTM model path -----\n",
    "if TTM_REVISION == 1:\n",
    "    TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
    "    # ----- TTM model branch -----\n",
    "    # For R1 models\n",
    "    if CONTEXT_LENGTH == 512:\n",
    "        TTM_MODEL_REVISION = \"main\"\n",
    "    elif CONTEXT_LENGTH == 1024:\n",
    "        TTM_MODEL_REVISION = \"1024_96_v1\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported CONTEXT_LENGTH for TTM_MODEL_PATH={TTM_MODEL_PATH}\")\n",
    "elif TTM_REVISION == 2:\n",
    "    TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "    # ----- TTM model branch -----\n",
    "    # For R2 models\n",
    "    if CONTEXT_LENGTH == 512:\n",
    "        TTM_MODEL_REVISION = \"main\"\n",
    "    elif CONTEXT_LENGTH == 1024:\n",
    "        TTM_MODEL_REVISION = \"1024-96-r2\"\n",
    "    elif CONTEXT_LENGTH == 1536:\n",
    "        TTM_MODEL_REVISION = \"1536-96-r2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported CONTEXT_LENGTH for TTM_MODEL_PATH={TTM_MODEL_PATH}\")\n",
    "else:\n",
    "    raise ValueError(\"Wrong TTM_REVISION. Stay tuned for future models.\")\n",
    "print(\"Chosen TTM model:\")\n",
    "print(f\"{TTM_MODEL_PATH}, revision = {TTM_MODEL_REVISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-2574695:t-129782190835200:data_handling.py:load_dataset:Dataset name: electricity, context length: 512, prediction length 96\n",
      "INFO:p-2574695:t-129782190835200:data_handling.py:load_dataset:Data lengths: train = 708, val = 234, test = 4837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "past_values 的形状: torch.Size([512, 321])\n",
      "past_values 的类型:<class 'torch.Tensor'>\n",
      "future_values 的形状: torch.Size([96, 321])\n",
      "future_values 的类型:<class 'torch.Tensor'>\n",
      "past_observed_mask 的形状: torch.Size([512, 321])\n",
      "past_observed_mask 的类型:<class 'torch.Tensor'>\n",
      "future_observed_mask 的形状: torch.Size([96, 321])\n",
      "future_observed_mask 的类型:<class 'torch.Tensor'>\n",
      "timestamp 的内容: 2016-10-14 01:00:00\n",
      "id 的内容: (0,)\n"
     ]
    }
   ],
   "source": [
    "# 打印导入数据集\n",
    "dataset = load_dataset('electricity', context_length=512, forecast_length=96, dataset_path=DATASET_PATH)\n",
    "# 直接打印数据集的类型和内容\n",
    "print(type(dataset))\n",
    "\n",
    "# 得到训练集\n",
    "train_dataset, val_dataset, test_dataset = dataset  # dataset 是加载的数据集\n",
    "\n",
    "# 获取测试集的一个示例\n",
    "test_index = 0  # 你可以修改为需要的索引\n",
    "test_sample = test_dataset[test_index]\n",
    "\n",
    "# 打印数据的形状\n",
    "for key, value in test_sample.items():\n",
    "    if isinstance(value, (np.ndarray, torch.Tensor)):\n",
    "        print(f\"{key} 的形状: {value.shape}\")\n",
    "        print(f\"{key} 的类型:{type(value)}\")\n",
    "    else:\n",
    "        print(f\"{key} 的内容: {value}\")  # 如果不是数组或张量，直接打印内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义评估指标\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def mase(y_true, y_pred, train, m=1):\n",
    "    # 计算训练集的绝对误差\n",
    "    train_errors = np.abs(np.array(train[m:]) - np.array(train[:-m]))  # 滞后 m\n",
    "    mae_train = np.mean(train_errors)  # 训练集的平均绝对误差\n",
    "    # 计算 MASE\n",
    "    return np.mean(np.abs(y_true - y_pred)) / mae_train if mae_train != 0 else np.nan\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局变量，用于存储最大内存占用\n",
    "max_cpu_memory = 0.0  \n",
    "max_gpu_memory = 0.0  \n",
    "def monitor_memory():\n",
    "    global max_cpu_memory, max_gpu_memory\n",
    "    process = psutil.Process(os.getpid())\n",
    "    while True:\n",
    "        # 获取当前 CPU 物理内存使用\n",
    "        cpu_memory = process.memory_info().rss / (1024 ** 2)  # 转换为 MB\n",
    "        max_cpu_memory = max(max_cpu_memory, cpu_memory)\n",
    "        \n",
    "        # 获取 GPU 最大内存使用\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.max_memory_reserved() / (1024 ** 2)  # 转为 MB\n",
    "            max_gpu_memory = max(max_gpu_memory, gpu_memory)\n",
    "        \n",
    "        # 每隔一秒采样一次\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "def zeroshot_eval(dataset_name, batch_size, context_length=512, forecast_length=96, prediction_filter_length=None):\n",
    "    torch.cuda.empty_cache()  # 清理缓存\n",
    "\n",
    "    global max_cpu_memory, max_gpu_memory\n",
    "    max_cpu_memory = 0.0  \n",
    "    max_gpu_memory = 0.0  \n",
    "    \n",
    "    # 启动监控线程\n",
    "    monitor_thread = threading.Thread(target=monitor_memory)\n",
    "    monitor_thread.daemon = True  # 设置为守护线程，在主线程结束时自动退出\n",
    "    monitor_thread.start()\n",
    "\n",
    "    if prediction_filter_length is not None:\n",
    "        if prediction_filter_length >= forecast_length:\n",
    "            raise ValueError(\n",
    "                \"`prediction_filter_length` should be less than the original `forecast_length` of the pre-trained TTM model.\"\n",
    "            )\n",
    "        # forecast_length = forecast_length - prediction_filter_length\n",
    "        forecast_length =  prediction_filter_length\n",
    "\n",
    "    # Get data\n",
    "    _, _, dset_test = load_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        context_length=context_length,\n",
    "        forecast_length=forecast_length,\n",
    "        # fewshot_fraction=1.0,\n",
    "        dataset_path=DATASET_PATH,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    if prediction_filter_length is None:\n",
    "        zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(TTM_MODEL_PATH, revision=TTM_MODEL_REVISION)\n",
    "    else:\n",
    "        if prediction_filter_length <= forecast_length:\n",
    "            zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=temp_dir,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # # 评估\n",
    "    # print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    # zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    # print(zeroshot_output)\n",
    "    \n",
    "    # 进行预测\n",
    "    start_time = time.time()\n",
    "    prediction_output = zeroshot_trainer.predict(dset_test)  # 获取预测输出\n",
    "    total_time = time.time() - start_time\n",
    "    predictions = prediction_output.predictions[0]\n",
    "    \n",
    "    # 获取标签\n",
    "    future_values = np.array([sample['future_values'] for sample in dset_test])  \n",
    "    # 根据 predictions 的长度切片 future_values\n",
    "    # future_values_selected = future_values[:len(predictions)]  \n",
    "    # 在缩短预测长度的时候，出现了预测样本数和真实样本数不一致的情况，这里做一些截断\n",
    "    min_length = min(len(future_values), len(predictions))\n",
    "    future_values = future_values[:min_length]\n",
    "    predictions =  predictions[:min_length]  \n",
    "    #计算指标\n",
    "    mse = mean_squared_error(future_values.reshape(-1), predictions.reshape(-1))\n",
    "    mae = mean_absolute_error(future_values.reshape(-1), predictions.reshape(-1))\n",
    "    mape = np.mean(np.abs((future_values - predictions) / future_values)) * 100  # 转为百分比\n",
    "    smape_value = smape(future_values, predictions)\n",
    "    mase_value = mase(future_values, predictions, future_values)  # 传入训练集（或验证集）\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"均方误差 (MSE):\", mse)\n",
    "    print(\"平均绝对误差 (MAE):\", mae)\n",
    "    print(\"平均绝对百分比误差 (MAPE):\", mape)\n",
    "    print(\"对称平均绝对百分比误差 (SMAPE):\", smape_value)\n",
    "    print(\"平均绝对误差比 (MASE):\", mase_value)\n",
    "\n",
    "    print(\"总运行时间: {:.2f} 秒\".format(total_time))\n",
    "    print(\"CPU 内存最大占用: {:.2f} MB\".format(max_cpu_memory))\n",
    "    print(\"GPU 内存最大占用: {:.2f} MB\".format(max_gpu_memory) if torch.cuda.is_available() else \"GPU不可用\")\n",
    "\n",
    "\n",
    "    # # plot\n",
    "    # plot_predictions(\n",
    "    #     model=zeroshot_trainer.model,\n",
    "    #     dset=dset_test,\n",
    "    #     plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "    #     plot_prefix=\"test_zeroshot\",\n",
    "    #     # indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "    #     channel=0,\n",
    "    # )\n",
    "    \n",
    "    # 清理临时目录\n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-2574695:t-129782190835200:data_handling.py:load_dataset:Dataset name: electricity, context length: 512, prediction length 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-2574695:t-129782190835200:data_handling.py:load_dataset:Data lengths: train = 803, val = 329, test = 4932\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差 (MSE): 0.79112315\n",
      "平均绝对误差 (MAE): 0.71543497\n",
      "平均绝对百分比误差 (MAPE): 344.65527534484863\n",
      "对称平均绝对百分比误差 (SMAPE): 125.94932317733765\n",
      "平均绝对误差比 (MASE): 0.98382115\n",
      "总运行时间: 219.35 秒\n",
      "CPU 内存最大占用: 33603.12 MB\n",
      "GPU 内存最大占用: 48340.00 MB\n"
     ]
    }
   ],
   "source": [
    "zeroshot_eval(dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, batch_size=4,prediction_filter_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_finetune_eval(\n",
    "    dataset_name,\n",
    "    batch_size,\n",
    "    learning_rate=None,\n",
    "    context_length=512,\n",
    "    forecast_length=96,\n",
    "    fewshot_percent=5,\n",
    "    freeze_backbone=True,\n",
    "    num_epochs=50,\n",
    "    save_dir=OUT_DIR,\n",
    "    prediction_filter_length=None,\n",
    "):\n",
    "    torch.cuda.empty_cache()  # 清理缓存\n",
    "    \n",
    "    global max_cpu_memory, max_gpu_memory\n",
    "    max_cpu_memory = 0.0  \n",
    "    max_gpu_memory = 0.0  \n",
    "    # 启动监控线程\n",
    "    monitor_thread = threading.Thread(target=monitor_memory)\n",
    "    monitor_thread.daemon = True  # 设置为守护线程，在主线程结束时自动退出\n",
    "    monitor_thread.start()\n",
    "    \n",
    "    out_dir = os.path.join(save_dir, dataset_name)\n",
    "\n",
    "    # print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "\n",
    "    if prediction_filter_length is not None:\n",
    "        if prediction_filter_length >= forecast_length:\n",
    "            raise ValueError(\n",
    "                \"`prediction_filter_length` should be less than the original `forecast_length` of the pre-trained TTM model.\"\n",
    "            )\n",
    "        # forecast_length = forecast_length - prediction_filter_length\n",
    "        forecast_length = prediction_filter_length\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "    dset_train, dset_val, dset_test = load_dataset(\n",
    "        dataset_name,\n",
    "        context_length,\n",
    "        forecast_length,\n",
    "        fewshot_fraction=fewshot_percent / 100,\n",
    "        dataset_path=DATASET_PATH,\n",
    "    )\n",
    "\n",
    "    # change head dropout to 0.7 for ett datasets\n",
    "    if \"ett\" in dataset_name:\n",
    "        if prediction_filter_length is None:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH, revision=TTM_MODEL_REVISION, head_dropout=0.7\n",
    "            )\n",
    "        elif prediction_filter_length <= forecast_length:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                head_dropout=0.7,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    else:\n",
    "        if prediction_filter_length is None:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "            )\n",
    "        elif prediction_filter_length <= forecast_length:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                TTM_MODEL_PATH,\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    if freeze_backbone:\n",
    "        print(\n",
    "            \"Number of params before freezing backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "        # Freeze the backbone of the model\n",
    "        for param in finetune_forecast_model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Count params\n",
    "        print(\n",
    "            \"Number of params after freezing the backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "    # Find optimal learning rate\n",
    "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
    "    if learning_rate is None:\n",
    "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
    "            finetune_forecast_model,\n",
    "            dset_train,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
    "\n",
    "    print(f\"Using learning rate = {learning_rate}\")\n",
    "    finetune_forecast_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"output\"),\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        dataloader_num_workers=8,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create the early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
    "    )\n",
    "    tracking_callback = TrackingCallback()\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
    "    )\n",
    "\n",
    "    finetune_forecast_trainer = Trainer(\n",
    "        model=finetune_forecast_model,\n",
    "        args=finetune_forecast_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val, \n",
    "        callbacks=[early_stopping_callback, tracking_callback],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
    "\n",
    "    # Fine tune\n",
    "    start_time = time.time();\n",
    "    finetune_forecast_trainer.train()\n",
    "\n",
    "    # # Evaluation\n",
    "    # print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
    "    # fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "    # print(fewshot_output)\n",
    "    # print(\"+\" * 60)\n",
    "\n",
    "    # 进行预测\n",
    "    prediction_output = finetune_forecast_trainer.predict(dset_test)  # 获取预测输出\n",
    "    total_time = time.time() - start_time\n",
    "    predictions = prediction_output.predictions[0]\n",
    "    # 获取标签\n",
    "    future_values = np.array([sample['future_values'] for sample in dset_test])  \n",
    "\n",
    "    #计算指标\n",
    "    mse = mean_squared_error(future_values.reshape(-1), predictions.reshape(-1))\n",
    "    mae = mean_absolute_error(future_values.reshape(-1), predictions.reshape(-1))\n",
    "    mape = np.mean(np.abs((future_values - predictions) / future_values)) * 100  # 转为百分比\n",
    "    smape_value = smape(future_values, predictions)\n",
    "    mase_value = mase(future_values, predictions, future_values)  # 传入训练集（或验证集）\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"均方误差 (MSE):\", mse)\n",
    "    print(\"平均绝对误差 (MAE):\", mae)\n",
    "    print(\"平均绝对百分比误差 (MAPE):\", mape)\n",
    "    print(\"对称平均绝对百分比误差 (SMAPE):\", smape_value)\n",
    "    print(\"平均绝对误差比 (MASE):\", mase_value)\n",
    "\n",
    "    print(\"总运行时间: {:.2f} 秒\".format(total_time))\n",
    "    print(\"CPU 内存最大占用: {:.2f} MB\".format(max_cpu_memory))\n",
    "    print(\"GPU 内存最大占用: {:.2f} MB\".format(max_gpu_memory) if torch.cuda.is_available() else \"GPU不可用\")\n",
    "\n",
    "    # plot\n",
    "    # plot_predictions(\n",
    "    #     model=finetune_forecast_trainer.model,\n",
    "    #     dset=dset_test,\n",
    "    #     plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "    #     plot_prefix=\"test_fewshot\",\n",
    "    #     # indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "    #     channel=0,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-2574695:t-129782190835200:data_handling.py:load_dataset:Dataset name: electricity, context length: 512, prediction length 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-2574695:t-129782190835200:data_handling.py:load_dataset:Data lengths: train = 401, val = 329, test = 4932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params before freezing backbone 805280\n",
      "Number of params after freezing the backbone 289696\n",
      "Using learning rate = 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1212' max='5050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1212/5050 01:25 < 04:31, 14.15 it/s, Epoch 12/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.875476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.876300</td>\n",
       "      <td>0.854696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.854766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.855906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>0.861160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.862338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.824700</td>\n",
       "      <td>0.871303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.885820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.787800</td>\n",
       "      <td>0.900694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.774400</td>\n",
       "      <td>0.904885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.770300</td>\n",
       "      <td>0.905434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.932961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrackingCallback] Mean Epoch Time = 4.248980005582173 seconds, Total Train Time = 87.2284848690033\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差 (MSE): 0.8085191\n",
      "平均绝对误差 (MAE): 0.74176455\n",
      "平均绝对百分比误差 (MAPE): 315.4524326324463\n",
      "对称平均绝对百分比误差 (SMAPE): 136.31083965301514\n",
      "平均绝对误差比 (MASE): 1.020028\n",
      "总运行时间: 291.94 秒\n",
      "CPU 内存最大占用: 33603.58 MB\n",
      "GPU 内存最大占用: 48340.00 MB\n"
     ]
    }
   ],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, batch_size=4, fewshot_percent=50, learning_rate=0.001,prediction_filter_length=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zql_ttm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhupengtian/miniconda3/envs/zql_moment/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zhupengtian/miniconda3/envs/zql_moment/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from momentfm import MOMENTPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "from momentfm.utils.utils import control_randomness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fl = 48\n",
    "batch_size = 24\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理部分\n",
    "class ECLDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecast_horizon: Optional[int] = fl,\n",
    "        data_path :str = '/home/zhupengtian/zhangqingliang/moment/data/electricity.csv' ,\n",
    "        data_split: str = \"train\",\n",
    "        data_stride_len: int = 1,\n",
    "        task_name: str = \"forecasting\",\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "\n",
    "        self.seq_len = 512\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.full_file_path_and_name = data_path\n",
    "        self.data_split = data_split\n",
    "        self.data_stride_len = data_stride_len\n",
    "        self.task_name = task_name\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Read data\n",
    "        self._read_data()\n",
    "        \n",
    "    def _get_borders(self, train_pct: float = 0.1, val_pct: float = 0.05, test_pct: float = 0.85):\n",
    "        # 计算数据集总长度\n",
    "        total_length = self.length_timeseries_original\n",
    "\n",
    "        # 计算各个数据集的结束位置\n",
    "        n_train = int(total_length * train_pct)\n",
    "        n_val = int(total_length * val_pct)\n",
    "        n_test = int(total_length * test_pct)\n",
    "\n",
    "        # 确保数据集划分之和不超过总数据集长度\n",
    "        assert n_train + n_val + n_test <= total_length, \"Total split exceeds the length of the dataset.\"\n",
    "\n",
    "        # 计算边界\n",
    "        train_end = n_train\n",
    "        val_end = n_train + n_val\n",
    "        test_start = val_end\n",
    "        test_end = test_start + n_test\n",
    "\n",
    "        # 划分数据集的切片\n",
    "        train = slice(0, train_end)\n",
    "        val = slice(train_end, val_end)\n",
    "        test = slice(test_start, test_end)\n",
    "\n",
    "        return train,test\n",
    "\n",
    "    def _read_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df = pd.read_csv(self.full_file_path_and_name)\n",
    "        self.length_timeseries_original = df.shape[0]\n",
    "        self.n_channels = df.shape[1] - 1\n",
    "\n",
    "        df.drop(columns=[\"date\"], inplace=True)\n",
    "        df = df.infer_objects(copy=False).interpolate(method=\"cubic\")\n",
    "\n",
    "        data_splits = self._get_borders()\n",
    "\n",
    "        train_data = df[data_splits[0]]\n",
    "        self.scaler.fit(train_data.values)\n",
    "        df = self.scaler.transform(df.values)\n",
    "\n",
    "        if self.data_split == \"train\":\n",
    "            self.data = df[data_splits[0], :]\n",
    "        elif self.data_split == \"test\":\n",
    "            self.data = df[data_splits[1], :]\n",
    "\n",
    "        self.length_timeseries = self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_start = self.data_stride_len * index\n",
    "        seq_end = seq_start + self.seq_len\n",
    "        input_mask = np.ones(self.seq_len)\n",
    "\n",
    "        if self.task_name == \"forecasting\":\n",
    "            pred_end = seq_end + self.forecast_horizon\n",
    "\n",
    "            if pred_end > self.length_timeseries:\n",
    "                pred_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.forecast_horizon\n",
    "                seq_start = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "            forecast = self.data[seq_end:pred_end, :].T\n",
    "\n",
    "            return timeseries, forecast, input_mask\n",
    "\n",
    "        elif self.task_name == \"imputation\":\n",
    "            if seq_end > self.length_timeseries:\n",
    "                seq_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "\n",
    "            return timeseries, input_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.task_name == \"imputation\":\n",
    "            return (self.length_timeseries - self.seq_len) // self.data_stride_len + 1\n",
    "        elif self.task_name == \"forecasting\":\n",
    "            return (\n",
    "                self.length_timeseries - self.seq_len - self.forecast_horizon\n",
    "            ) // self.data_stride_len + 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "# 计算 MSE (Mean Squared Error)\n",
    "def mse(trues, preds):\n",
    "    return np.mean((trues - preds) ** 2)\n",
    "\n",
    "# 计算 MAE (Mean Absolute Error)\n",
    "def mae(trues, preds):\n",
    "    return np.mean(np.abs(trues - preds))\n",
    "\n",
    "# 计算 SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(trues, preds):\n",
    "    numerator = np.abs(trues - preds)\n",
    "    denominator = (np.abs(trues) + np.abs(preds)) / 2\n",
    "    return 200 * np.mean(numerator / denominator)\n",
    "\n",
    "# 计算 MAPE (Mean Absolute Percentage Error)\n",
    "def mape(trues, preds):\n",
    "    return np.mean(np.abs((trues - preds) / trues)) * 100\n",
    "\n",
    "# 计算 MASE (Mean Absolute Scaled Error)\n",
    "def mase(trues, preds, historical_values):\n",
    "    # 计算基准模型（例如：上一时刻值）误差\n",
    "    naive_error = np.abs(trues[1:] - trues[:-1])\n",
    "    forecast_error = np.abs(trues - preds)\n",
    "    return np.mean(forecast_error[1:]) / np.mean(naive_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 导入模型\n",
    "model = MOMENTPipeline.from_pretrained(\n",
    "    \"/home/zhupengtian/zhangqingliang/models/MOMENT-1-large\", \n",
    "    model_kwargs={\n",
    "        'task_name': 'forecasting',\n",
    "        'forecast_horizon': fl,\n",
    "        'head_dropout': 0.1,\n",
    "        'weight_decay': 0,\n",
    "        'freeze_encoder': True, # Freeze the patch embedding layer\n",
    "        'freeze_embedder': True, # Freeze the transformer encoder\n",
    "        'freeze_head': False, # The linear forecasting head must be trained\n",
    "    },\n",
    "    local_files_only=True,  # Whether or not to only look at local files (i.e., do not try to download the model).\n",
    ")\n",
    "model.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载数据集，设置训练参数\n",
    "# Set random seeds for PyTorch, Numpy etc.\n",
    "control_randomness(seed=13) \n",
    "\n",
    "# Load data\n",
    "train_dataset = ECLDataset(data_split=\"train\", random_seed=13, forecast_horizon=fl)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = ECLDataset(data_split=\"test\", random_seed=13, forecast_horizon=fl)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 训练周期数\n",
    "cur_epoch = 0\n",
    "max_epoch = 1\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the loss function to the GPU\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a OneCycleLR scheduler\n",
    "max_lr = 1e-4\n",
    "total_steps = len(train_loader) * max_epoch\n",
    "scheduler = OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.3)\n",
    "\n",
    "# Gradient clipping value\n",
    "max_norm = 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 微调模型（预测头）\n",
    "while cur_epoch < max_epoch:\n",
    "    losses = []\n",
    "    for timeseries, forecast, input_mask in tqdm(train_loader, total=len(train_loader)):\n",
    "        # Move the data to the GPU\n",
    "        timeseries = timeseries.float().to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        forecast = forecast.float().to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x_enc=timeseries, input_mask=input_mask)\n",
    "        \n",
    "        loss = criterion(output.forecast, forecast)\n",
    "\n",
    "        # Scales the loss for mixed precision training\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    print(f\"Epoch {cur_epoch}: Train loss: {average_loss:.3f}\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    cur_epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the test split\n",
    "trues, preds, histories, losses = [], [], [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for timeseries, forecast, input_mask in tqdm(test_loader, total=len(test_loader)):\n",
    "    # Move the data to the GPU\n",
    "        timeseries = timeseries.float().to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        forecast = forecast.float().to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x_enc=timeseries, input_mask=input_mask)\n",
    "        \n",
    "        loss = criterion(output.forecast, forecast)                \n",
    "        losses.append(loss.item())\n",
    "\n",
    "        trues.append(forecast.detach().cpu().numpy())\n",
    "        preds.append(output.forecast.detach().cpu().numpy())\n",
    "        histories.append(timeseries.detach().cpu().numpy())\n",
    "\n",
    "losses = np.array(losses)\n",
    "average_loss = np.average(losses)\n",
    "model.train()\n",
    "\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "histories = np.concatenate(histories, axis=0)\n",
    "\n",
    "# 计算各项指标\n",
    "mse = mse(trues, preds)\n",
    "mae = mae(trues, preds)\n",
    "smape = smape(trues, preds)\n",
    "mape = mape(trues, preds)\n",
    "historical_values = trues[:-1]  # 用上一时刻的值作为基准\n",
    "mase = mase(trues, preds, historical_values)\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"SMAPE: {smape:.6f}\")\n",
    "print(f\"MAPE: {mape:.6f}\")\n",
    "print(f\"MASE: {mase:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zql_moment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
